\section{Clause-containment learning for E-MAJSAT}
\label{sect:erssat-technique}

Consider an E-MAJSAT formula $\Qf = \exists X,\random{} Y.\pf$.
To obtain the satisfying probability of $\Qf$,
it suffices to enumerate every assignment $\as$ over $X$ and calculate the corresponding conditional satisfying probability $\spb{\pcf{\Qf}{\as}}$.
Clearly, the above brute-force approach is computationally expensive.
Motivated by the idea of clause selection discussed above,
we propose \textit{clause-containment learning} to prune the search space.
The proposed learning technique deduces useful information after each trial of an assignment $\as$ over $X$.
The learnt information is recorded as blocking clauses to avoid wasteful exploration and thus accelerates the search process.
The proposed learning technique is based on the following key observation.

\begin{proposition}
    \label{prop:erssat-contain}
    Given an E-MAJSAT formula $\Qf=\exists X,\random{} Y.\pf(X,Y)$ and two assignments $\as_1,\as_2$ over $X$,
    we have:
    \begin{align*}
        (\pcf{\pf}{\as_2}\limply\pcf{\pf}{\as_1})\limply\spb{\pcf{\Qf}{\as_2}}\leq\spb{\pcf{\Qf}{\as_1}}.
    \end{align*}
\end{proposition}

Inspired by~\cref{prop:erssat-contain},
we propose clause-containment learning based on clause selection.
After cofactoring $\pf$ with an arbitrary assignment $\as_1$ over $X$,
a set of clauses $\pcf{\pf}{\as_1}$ is selected.
For any other assignment $\as_2$ selecting all clauses from $\pcf{\pf}{\as_1}$,
i.e., $\pcf{\pf}{\as_1}\subseteq\pcf{\pf}{\as_2}$,
we have $\pcf{\pf}{\as_2}\limply\pcf{\pf}{\as_1}$.
Therefore, $\spb{\pcf{\Qf}{\as_2}}\leq\spb{\pcf{\Qf}{\as_1}}$ holds according to~\cref{prop:erssat-contain}.
Since the satisfying probability $\spb{\pcf{\Qf}{\as_2}}$ is not greater than $\spb{\pcf{\Qf}{\as_1}}$,
the assignment $\as_2$ is not worth trying.
For all such assignments,
they should be blocked after $\as_1$ has been explored.

The core concept of the clause-containment learning is to avoid every unexplored assignment $\as_2$ that selects a clause set $\pcf{\pf}{\as_2}$ containing another clause set $\pcf{\pf}{\as_1}$ selected by an explored assignment $\as_1$.
To block the assignment $\as_2$,
we enforce at least one of the clauses in $\pcf{\pf}{\as_1}$ to be deselected.
Recall that the selection variable $\sv{C}$ of clause $C$ valuates to $\bot$ if and only if $C$ is deselected.
Therefore, the disjunction of the negation of the selection variables for the clauses in $\pcf{\pf}{\as_1}$ is deduced as a learnt clause to record this information.
The above idea gives rise to~\cref{alg:erssat} for E-MAJSAT formulas.
(\Cref{code:erssat-subsume-table,code:erssat-minimal-clauses,code:erssat-subsume-clauses,code:erssat-discard-literals} describe the clause-strengthening heuristics of the proposed algorithm, which will be discussed later.)

\begin{algorithm}[p]
    \caption{Solving exist-random quantified SSAT (E-MAJSAT) formulas}
    \label{alg:erssat}
    \begin{algorithmic}[1]
        \REQUIRE $\Qf=\exists X,\random{} Y.\pf(X,Y)$
        \ENSURE $\spb{\Qf}$
        \STATE $\select(X,S) := \bigwedge\limits_{C\in\pf}(\sv{C}\equiv\lnot\cx)\land\bigwedge\limits_{\text{pure }l: \vl{l}\in X}l$\label{code:erssat-init-select}
        \STATE $prob := 0$
        \STATE $\texttt{s-table} := \texttt{BuildSubsumptionTable}(\pf)$\label{code:erssat-subsume-table}
        \WHILE{($\sat{\select}$)}
        \STATE $\as := \model{\select}$ (discarding the selection variables)
        \IF{($\sat{\pcf{\pf}{\as}}$)}
        \STATE $\as' := \texttt{SelectMinimalClauses}(\pf,\select)$\label{code:erssat-minimal-clauses}
        \STATE $\varphi := \texttt{RemoveSubsumedClauses}(\pcf{\pf}{\as'},\texttt{s-table})$\label{code:erssat-subsume-clauses}
        \STATE $prob := \max\{prob,\texttt{ComputeWeight}(\random{} Y.\varphi)\}$\label{code:erssat-wmc}
        \STATE $C_S := \bigvee\limits_{C\in\varphi}\lnot\sv{C}$
        \STATE $C_L := \texttt{DiscardLiterals}(\pf,C_S,prob)$\label{code:erssat-discard-literals}
        \ELSE
        \STATE $C_L := \texttt{MinimalConflicting}(\pf,\as)$
        \ENDIF
        \STATE $\select := \select \land C_L$
        \ENDWHILE
        \RETURN $prob$
    \end{algorithmic}
\end{algorithm}

The algorithm employs two SAT solvers:
one works on the matrix $\pf(X,Y)$ of the input formula,
and the other works on the selection relation $\select(X,S)$ for clauses in $\pf(X,Y)$.
Using the definition of selection variables,
we initialize the selection relation and assert the literals of pure $X$ variables in~\cref{code:erssat-init-select}.
If a variable $e$ in $X$ is pure in $\pf$,
assigning the literals of $e$ to $\top$ deselects the clauses containing $e$,
and does not affect other clauses.
Because the conditional satisfying probability is greater if less clauses are selected,
we can safely assert pure $X$ variables without missing the optimizing solution.

The selection relation is used to select different assignments $\as$ over $X$.
If $\pcf{\pf}{\as}$ is satisfiable,
a weighted model counter is called to compute the conditional satisfying probability $\spb{\random{} Y.\pcf{\pf}{\as}}$.
The blocking clause $C_L$ derived from the containment-learning technique is then conjoined with $\select$ to prevent clauses in $\pcf{\pf}{\as}$ being simultaneously selected again.

On the other hand, suppose $\pcf{\pf}{\as}$ is unsatisfiable.
We can deduce a conjunction of literals from $\as$ responsible for the conflict by using a SAT solver to analyze the conflict~\cite{Een2003Solver,Een2003Incremental}.
In general, the conjunction of literals may not be minimal,
meaning that some literals can be discarded and the conflict remains unaffected.
The subroutine $\texttt{MinimalConflicting}$ makes the conjunction of literals responsible for the conflict minimal as follows.
For each literal $l$ in the conjunction,
temporarily drop $l$ and check whether $\pf(X,Y)$ is still unsatisfiable.
If it is unsatisfiable, discard $l$; otherwise, keep $l$ in the conjunction.
Repeating the above process for every literal makes the conjunction minimal.
Complementing the minimal conjunction of literals yields a learnt clause,
which is then conjoined with the selection relation to block assignments that make $\pf$ unsatisfiable.

When the selection relation becomes unsatisfiable,
it indicates that the space spanned by variables $X$ has been completely searched.
The algorithm will return the encountered maximum conditional satisfying probability,
which equals the satisfying probability of the input E-MAJSAT formula.

\begin{example}
    \label{ex:erssat-basic}
    \begin{table}[t]
        \centering
        \caption{Solving process of~\cref{alg:erssat} on~\cref{ex:erssat-basic}}
        \label{tbl:erssat-solve-example}
        \small
        \begin{tabular}{c|c|c|c|c}
            Assignment                            & Selected Clauses & $\spb{\pcf{\Qf}{\as}}$ & Learnt Clause                & LB     \\
            \hline
            $\as_1=\lnot e_1 \lnot e_2 \lnot e_3$ & $\{C_1,C_2\}$    & $0.75$                 & $(\lnot s_1 \lor \lnot s_2)$ & $0.75$ \\
            $\as_2=\lnot e_1 e_2 \lnot e_3$       & $\{C_1\}$        & $0.75$                 & $(\lnot s_1)$                & $0.75$ \\
            $\as_3=e_1 e_2 \lnot e_3$             & $\{C_4\}$        & $0.5$                  & $(\lnot s_4)$                & $0.75$ \\
            $\as_4=e_1 e_2 e_3$                   & $\{C_3\}$        & $0.75$                 & $(\lnot s_3)$                & $0.75$ \\
            $\as_5=e_1 \lnot e_2 e_3$             & $\{\}$           & $1$                    & $()$                         & $1$
        \end{tabular}
    \end{table}

    Continuing~\cref{ex:erssat-select},
    we show how~\cref{alg:erssat} (without the clause-strengthening heuristics) solves the E-MAJSAT instance
    \begin{align*}
        \Qf=\exists e_1,\exists e_2,\exists e_3,\random{0.5} r_1,\random{0.5} r_2,\random{0.5} r_3.\pf.
    \end{align*}

    The solving process is summarized in~\cref{tbl:erssat-solve-example}.
    We first explore $\as_1=\lnot e_1 \lnot e_2 \lnot e_3$, which selects $C_1$ and $C_2$.
    The algorithm derives $\spb{\random{0.5} r_1,\random{0.5} r_2,\random{0.5} r_3.\pcf{\pf}{\as_1}}=0.75$ by invoking a weighted model counter in~\cref{code:erssat-wmc}.
    The learnt clause $C_L=(\lnot s_1 \lor \lnot s_2)$ is conjoined with $\select$ to prevent $C_1$ and $C_2$ being selected simultaneously again.

    Suppose the second explored assignment $\as_2$ is $\lnot e_1 e_2 \lnot e_3$,
    which selects $C_1$.
    The weighted model counter returns $\spb{\random{0.5} r_1,\random{0.5} r_2,\random{0.5} r_3.\pcf{\pf}{\as_2}}=0.75$,
    and the learnt clause $C_L=(\lnot s_1)$ is conjoined with $\select$ to prevent $C_1$ being selected again.

    Let the third tried assignment $\as_3$ be $e_1 e_2 \lnot e_3$,
    which selects $C_4$.
    The weighted model counter gives $\spb{\random{0.5} r_1,\random{0.5} r_2,\random{0.5} r_3.\pcf{\pf}{\as_3}}=0.5$,
    and the learnt clause $C_L=(\lnot s_4)$ is conjoined with $\select$ to prevent $C_4$ being selected again.

    Let the fourth tried assignment $\as_4$ be $e_1 e_2 e_3$,
    which selects $C_3$.
    The conditional satisfying probability $\spb{\random{0.5} r_1,\random{0.5} r_2,\random{0.5} r_3.\pcf{\pf}{\as_4}}$ equals $0.75$,
    and the learnt clause $C_L=(\lnot s_3)$ is conjoined with $\select$ to prevent $C_3$ being selected again.

    Suppose the fifth tried assignment $\as_5$ is $e_1 \lnot e_2 e_3$,
    which deselects every clause, making $\pcf{\pf}{\as_5}=\top$ and
    $\spb{\random{0.5} r_1,\random{0.5} r_2,\random{0.5} r_3.\pcf{\pf}{\as_5}}=1$.
    Since there is no selected clause,
    the learnt clause $C_L$ is empty,
    and the selection relation becomes unsatisfiable after being conjoined with an empty clause.
    The unsatisfiability of the selection relation reveals that the space spanned by variables $X$ has been exhaustively searched,
    and the algorithm returns the satisfying probability, which equals $1$, of the E-MAJSAT instance.

    For approximate SSAT solving, suppose the procedure is forced to terminate right after $\as_3^+$ is explored.
    Although the space spanned by variables $X$ has not been fully searched yet,
    we can conclude that the satisfying probability is at least $0.75$ because $\as_1$ or $\as_2$ is a witness.
\end{example}

\subsection{Clause-strengthening heuristics}
The efficiency of~\cref{alg:erssat} is greatly affected by the strength of the learnt clauses.
We introduce three heuristics,
\textit{minimal clause selection},
\textit{clause subsumption}, and
\textit{partial assignment pruning},
to strengthen the learnt clauses deduced by the proposed learning technique.
In~\cref{alg:erssat},
the clause-strengthening heuristics are executed by subroutines
$\texttt{SelectMinimalClauses}$ (in~\cref{code:erssat-minimal-clauses}),
$\texttt{RemoveSubsumedClauses}$ (in~\cref{code:erssat-subsume-clauses}), and
$\texttt{DiscardLiterals}$ (in~\cref{code:erssat-discard-literals}),
to be detailed in the following three parts, respectively.
%Examples~\ref{ex:minimal},~\ref{ex:subsume}, and ~\ref{ex:partial} will be used to explain the three techniques.
%(Please refer to Examples~\ref{ex:minimal},~\ref{ex:subsume},~\ref{ex:partial} in the supplement.)

\subsubsection{Minimal clause selection}
As discussed before,
the selection relation $\select(X,S)$ is in charge of choosing an assignment $\as$ over variables $X$ and thus selects a set of clauses from the matrix $\pf(X,Y)$.
However, the set of selected clauses may not be minimal,
meaning that it is possible for another assignment $\as'$ to select a set of clauses contained in that selected by $\as$, i.e., $\pcf{\pf}{\as'}\subset\pcf{\pf}{\as}$.
Notice that the length of a learnt clause equals the number of selected clauses.
Therefore, selecting fewer clauses gives a stronger learnt clause,
as well as a higher conditional satisfying probability.

Starting from a set of initially selected clauses $\pcf{\pf}{\as}$,
the first heuristic \textit{minimal clause selection} decreases the number of selected clauses by making the set of initially selected clauses minimal as follows.
A learnt clause $C_S=\bigvee\limits_{C\in\pcf{\pf}{\as}}\lnot\sv{C}$ is conjoined with the selection relation $\select(X,S)$ to avoid clauses in $\pcf{\pf}{\as}$ being selected simultaneously again.
A SAT solver is invoked to solve $\select\land C_S$ under an assumption $\mu=\bigwedge\limits_{C\notin\pcf{\pf}{\as}}\lnot\sv{C}$.
The assumption $\mu$ prevents originally deselected clauses being selected.
If $\select\land C_S$ under the assumption $\mu$ is satisfied by some assignment $\as'$ over $X$,
the set of clauses selected by $\as'$ must be a proper subset of that selected by $\as$.
On the other hand, if $\select\land C_S$ under the assumption $\mu$ is unsatisfiable,
then the set of selected clauses is minimal.
To make the set of initially selected clauses minimal,
the above operation is repeated until the selection relation becomes unsatisfiable.

The subroutine $\texttt{SelectMinimalClauses}$ for the technique is described in~\cref{alg:erssat-minimal}.
The following example illustrates how this technique improves the solving efficiency.

\begin{algorithm}[ht]
    \caption{Subroutine of~\cref{alg:erssat}: \texttt{SelectMinimalClauses}}
    \label{alg:erssat-minimal}
    \begin{algorithmic}[1]
        \REQUIRE The matrix $\pf(X,Y)$ and selection relation $\select(X,S)$
        \ENSURE An assignment $\as'$ that selects a minimal set of clauses from $\pf$
        \REPEAT
        \STATE $\as' := \model{\select}$ (discarding the selection variables)
        \STATE $C_S := \bigvee\limits_{C\in\pcf{\pf}{\as'}}\lnot\sv{C}$
        \STATE $\select := \select \land C_S$
        \STATE $\mu := \bigwedge\limits_{C\notin\pcf{\pf}{\as'}}\lnot\sv{C}$
        \UNTIL{($\unsat{\pcf{\select}{\mu}}$)}
        \RETURN $\as'$
    \end{algorithmic}
\end{algorithm}

\begin{example}
    \label{ex:erssat-minimal}
    Continue~\cref{ex:erssat-basic}.
    The first tried assignment $\as_1=\lnot e_1 \lnot e_2 \lnot e_3$ selects $C_1$ and $C_2$.
    The set of selected clauses is made minimal as follows.
    The subroutine $\texttt{SelectMinimalClauses}$ conjoins the selection relation with a learnt clause $(\lnot \sv{1} \lor \lnot \sv{2})$,
    which prevents clauses $C_1$ and $C_2$ being simultaneously selected again.
    The satisfiability of the selection relation $\select\land(\lnot \sv{1} \lor \lnot \sv{2})$ is tested under an assumption $\lnot \sv{3} \lnot \sv{4}$,
    which avoids the originally deselected clause $C_3$ and $C_4$ being selected.
    The formula is satisfied by the assignment $\as_2=\lnot e_1 e_2 \lnot e_3$, which only selects $C_1$.
    By repeating the above operation again,
    the assignment $\as_5=e_1 \lnot e_2 e_3$ is found,
    since it selects no clause.
    The conditional satisfying probability under $\as_5$, which equals $1$,
    is derived without invoking a weighted model counter.
    Compared to~\ref{ex:erssat-basic} where five assignments over $X$ were explored,
    the algorithm with the minimal clause selection technique finds the optimal assignment $\as_5$ with at most two SAT calls,
    thus greatly improves the computation efficiency.
\end{example}

\subsubsection{Clause subsumption}
The second heuristic, named \textit{clause subsumption},
decreases the length of a learnt clause via examining the subsumption relation among the selected clauses.
Recall that clause $C_1$ \textit{subsumes} clause $C_2$ if every literal appears in $C_1$ also appears in $C_2$.
Consider a CNF formula $C_1 \land C_2$ with $C_2$ subsumed by $C_1$.
It can be simplified to $C_1$ because $C_2$ is implied by $C_1$ due to the subsumption relation.

The subsumption relation among sub-clauses consisting of variables in $Y$ is constructed as a lookup table by the subroutine $\mathtt{BuildSubsumeTable}$.
The subroutine $\texttt{RemoveSubsumedClauses}$ simplifies the set of selected clauses $\pcf{\pf}{\as}$ by removing subsumed clauses.
A clause $C$ is removed from the set of selected clauses $\pcf{\pf}{\as}$ if $C^Y$ is subsumed by other selected sub-clauses.
We emphasize that, without cofactoring $\pf$ with the assignment $\as$,
the original clauses may not have the subsumption relation.
Cofactoring $\pf$ with the assignment $\as$ over variables in $X$ induces the subsumption relation between sub-clauses consisting of variables in $Y$.

The procedure $\texttt{RemoveSubsumedClauses}$ for the technique is outlined in~\cref{alg:erssat-subsume}.
It takes a set of selected clauses and a lookup table of subsumption relation as input,
and removes every subsumed clause via subroutine $\texttt{CheckNotSubsumed}$.
The following example explains how the subsumption relation shortens a learnt clause.

\begin{algorithm}[ht]
    \caption{Subroutine of~\cref{alg:erssat}: \texttt{RemoveSubsumedClauses}}
    \label{alg:erssat-subsume}
    \begin{algorithmic}[1]
        \REQUIRE The selected clauses $\pcf{\pf}{\as}$ and a subsumption table \texttt{s-table}
        \ENSURE A simplified clause set $\varphi$ without subsumed clauses
        \STATE $\varphi := \top$
        \FOR{($C\in\pcf{\pf}{\as}$)}
        \IF{($\texttt{CheckNotSubsumed}(C,\texttt{s-table})$)}
        \STATE $\varphi := \varphi \land C$
        \ENDIF
        \ENDFOR
        \RETURN $\varphi$
    \end{algorithmic}
\end{algorithm}

\begin{example}
    \label{ex:erssat-subsume}
    Continue~\cref{ex:erssat-basic}.
    Recall that the first tried assignment is $\as_1=\lnot e_1 \lnot e_2 \lnot e_3$.
    It selects $C_1$ and $C_2$.
    Because $C_1^Y$ subsumes $C_2^Y$,
    $C_2$ is removed from $\pcf{\pf}{\as_1}$, yielding $\varphi=C_1$.
    A weighted model counter computes $\spb{\random{0.5}Y.\varphi}=0.75$,
    and the learnt clause $C_L=(\lnot \sv{1})$ is conjoined with $\select$ to prevent $C_1$ being selected again.
    Compared to~\cref{ex:erssat-basic}, with the help of clause subsumption,
    the learnt clause $(\lnot s_1)$ deduced from $\as_1$ is stronger than its counterpart $(\lnot s_1 \lor \lnot s_2)$ in~\cref{ex:erssat-basic},
    and therefore avoids a fruitless trail of the assignment $\as_2=\lnot e_1 e_2 \lnot e_3$.
\end{example}

\subsubsection{Partial assignment pruning}
To illustrate the third heuristic \textit{partial assignment pruning},
we first take a closer look at a learnt clause deduced by the proposed clause-containment learning.
Given a matrix $\pf(X,Y)$ and an assignment $\as$ over $X$,
a learnt clause is a disjunction of the negated selection variables of the selected clauses.
For each selected clause $C$,
if the selection variable $\sv{C}$ is substituted by its definition $\sv{C}\equiv\lnot\cx$,
the learnt clause $C_L$ becomes a disjunction of the sub-clauses $\cx$,
i.e., $C_L=\bigvee\limits_{C\in\pcf{\pf}{\as}}\cx$.

For instance, in~\cref{ex:erssat-basic},
the learnt clause deduced from the assignment $\as_1=\lnot e_1 \lnot e_2 \lnot e_3$,
which selects clauses $C_1$ and $C_2$,
is $(\lnot s_1 \lor \lnot s_2)=(e_1 \lor e_2)$,
and the current maximum satisfying probability equals $0.75$.
This learnt clause blocks two assignments, $\as_1$ and $\lnot e_1 \lnot e_2 e_3$.
The assignment $\lnot e_1 \lnot e_2 e_3$ is blocked
because it selects clauses $C_1$, $C_2$, and $C_4$,
and clauses $C_1$ and $C_2$ have been selected previously.

A learnt clause can be strengthened if some literal in the clause is discarded.
In the above example,
the learnt clause $(e_1 \lor e_2)$ can be strengthened by discarding literal $e_2$.
The resulted learnt clause $(e_1)$ blocks any assignment assigning $e_1$ to $\bot$.
To see why these assignments are blocked,
observe that assigning $e_1$ to $\bot$ selects clause $C_1$.
Although the set $\{C_1\}$ does not contain the set $\{C_1,C_2\}$,
the conditional satisfying probability is bounded from above by the probability of the sub-clause $(r_1 \lor r_2)$,
which equals $0.75$.
Since the current maximum satisfying probability equals $0.75$ already,
it is fruitless to explore assignments whose conditional satisfying probabilities are no greater than $0.75$. Therefore, $e_1$ is forced to be $\top$.

On the other hand,
literal $e_1$ cannot be discarded from the learnt clause $(e_1 \lor e_2)$.
If $e_1$ is discarded,
the resulted learnt clause $(e_2)$ blocks any assignment assigning $e_2$ to $\bot$.
However, assigning $e_2$ to $\bot$ selects no clause,
and hence the upper bound of the conditional satisfying probability equals $1$.
Since $1$ is greater than the current maximum satisfying probability $0.75$,
we have to explore assignments that map $e_2$ to $\bot$.
In fact, there exists an assignment $e_1 \lnot e_2 \lnot e_3$ whose conditional satisfying probability equals $1$,
greater than the current maximum satisfying probability.
As a result, $e_1$ cannot be discarded.

From the above illustration,
we observe that a learnt clause can be strengthened as follows.
First, temporarily discard some literal $l$ from a learnt clause.
Second, invoke a weighted model counter to compute the conditional satisfying probability contributed by the selected clauses.
Third, compare the probability to the current maximum satisfying probability.
If the probability is no greater, discard literal $l$;
otherwise, keep $l$ in the learnt clause.
Fourth, repeat the above steps for other literals.

The subroutine $\texttt{DiscardLiterals}$ of the technique to discard literals from a learnt clause is outlined in~\cref{alg:erssat-partial}.
It substitutes selection variables in a learnt clause $C_S$ by their definitions in~\cref{code:erssat-partial-replace}.
Then it iterates through each literal $l \in C_L$ and invokes a weighted model counter to check whether literal $l$ can be discarded in~\crefrange{code:erssat-partial-drop-start}{code:erssat-partial-drop-end}.
Notice that the current maximum satisfying probability is \textit{not} updated by the value obtained by weighted model counting,
because the value only reflects an upper bound for satisfying probabilities under the partial assignment $\nu$.

\begin{algorithm}[ht]
    \caption{Subroutine of~\cref{alg:erssat}: \texttt{DiscardLiterals}}
    \label{alg:erssat-partial}
    \begin{algorithmic}[1]
        \REQUIRE The matrix $\phi$, a learnt clause $C_S$, and the current maximum $prob$
        \ENSURE A strengthened learnt clause $C_L$
        \STATE $C_L := \bigvee\limits_{\sv{C}\in C_S}\cx$\label{code:erssat-partial-replace}
        \FOR{($l\in C_L$)}\label{code:erssat-partial-drop-start}
        \STATE $\nu := \bigwedge\limits_{k\in C_L\setminus\{l\}}\lnot k$
        \IF{($\texttt{ComputeWeight}(\random{}Y.\pcf{\pf}{\nu})\leq prob$)}
        \STATE $C_L := C_L \setminus \{l\}$\label{code:erssat-partial-drop-end}
        \ENDIF
        \ENDFOR
        \RETURN $C_L$
    \end{algorithmic}
\end{algorithm}

\begin{example}
    \label{ex:erssat-partial}
    Continue~\cref{ex:erssat-basic}.
    As discussed above, the learnt clause $(e_1 \lor e_2)$ deduced from the assignment $\as_1=\lnot e_1 \lnot e_2 \lnot e_3$ is strengthen to $(e_1)$ by the partial assignment pruning technique,
    and thus prevents a fruitless trail of the assignment $\as_2=\lnot e_1 e_2 \lnot e_3$.
\end{example}

To summarize, the above three clause-strengthening heuristics are designed based on different reasoning strategies:
the minimal clause selection technique strengthens the learnt clause by invoking additional SAT calls to solve the selection relation to select a minimal set of clauses;
the clause subsumption technique builds a lookup table to record the subsumption relation and removes subsumed clauses;
the partial assignment pruning technique utilizes the current maximum conditional satisfying probability,
and invokes extra calls to a weighted model counter to test whether it is feasible to discard some literal in a learnt clause.
Benefiting from the three clause-strengthening heuristics,
the efficiency of the proposed algorithm is improved over certain formulas,
as will be shown in our experiments.

We emphasize two more strengths of the proposed algorithm.
First, during the computation,
the proposed algorithm keeps deriving lower bounds for the satisfying probability,
and the bounds gradually converge to the final answer.
Therefore, in contrast to previous DPLL-based methods~\cite{Majercik1998,Majercik2003,Majercik2005},
the proposed algorithm can be easily modified to solve \textit{approximate SSAT} by returning the greatest encountered lower bound upon timeout.
Second, the proposed algorithm is efficient in memory usage,
since it stores the learnt information compactly via selection variables,
and the weighted model counting is invoked on selected clauses,
whose sizes are typically much smaller than that of original matrix.
%In our empirical evaluation, the selected clauses on average account for less than $20\%$ of the entire CNF formula.
%As will be clear from our experimental data, the proposed algorithm improves memory efficiency by more than one order of magnitude compared to previous approaches.

%We emphasize that, the proposed three clause-strengthening heuristics are orthogonal to each other. Simultaneously enabling all of them gives the enhanced algorithm in Figure~\ref{fig:enhance}.

\subsection{Implementation details}
We discuss some details about our implementation of the proposed algorithm.
\Cref{alg:erssat} involves satisfiability testing and weighted model counting.
In principle, any SAT solver and exact weighted model counter can be plugged into the procedure.
Therefore, \cref{alg:erssat} may benefit directly from the advancement of SAT solving and model counting.

Specifically in our implementation,
we adopt \texttt{Minisat-2.2}~\cite{Een2003Solver} for SAT solving.
We experimented with \cachet~\cite{Sang2004,Sang2005ModelCounting} for weighted model counting,
but the overall performance was not satisfactory.
Because the clauses selected by different assignments typically share a portion of identical sub-formulas,
the results of these sub-formulas should be cached to avoid repeated computations.
Ideally, this improvement can be achieved by tightly integrating \cachet into our implementation,
instead of treating it as a black box.
However, we resort to \textit{binary decision diagram} (BDD) for our weighted model counting to achieve the \textit{formula caching} effect.

The selected clauses in a formula are first transformed into a two-level circuit by \disjoin-ing literals in each clause and \conjoin-ing the outputs of each \disjoin gate.
A BDD is then constructed based on the resulted circuit.
Weighted model counting of the original formula is done by traversing the BDD~\cite{LeeTC18ProbDesign}.
We use the well-developed BDD package \texttt{CUDD}~\cite{CUDD},
which caches BDD nodes during building BDDs for different formulas.
Thus the formula caching effect,
as well as garbage collection which removes unreferenced nodes,
is automatically achieved by \cudd.

Our implementation was integrated in the \abc~\cite{ABC}environment,
which provides all the facilities including SAT solving, circuit construction, and BDD computation mentioned above.